/* This file is part of SLICC (Specification Language for Implementing
   Cache Coherence), a component of the Multifacet GEMS (General
   Execution-driven Multiprocessor Simulator) software toolset
   originally developed at the University of Wisconsin-Madison.

   SLICC was originally developed by Milo Martin with substantial
   contributions from Daniel Sorin.

   Ruby was originally developed primarily by Milo Martin and Daniel
   Sorin with contributions from Ross Dickson, Carl Mauer, and Manoj
   Plakal.

   Substantial further development of Multifacet GEMS at the
   University of Wisconsin was performed by Alaa Alameldeen, Brad
   Beckmann, Ross Dickson, Pacia Harper, Milo Martin, Michael Marty,
   Carl Mauer, Kevin Moore, Manoj Plakal, Daniel Sorin, Min Xu, and
   Luke Yen.

   Additional development was performed at the University of
   Pennsylvania by Milo Martin.

   --------------------------------------------------------------------

   Copyright (C) 1999-2005 by Mark D. Hill and David A. Wood for the
   Wisconsin Multifacet Project.  Contact: gems@cs.wisc.edu
   http://www.cs.wisc.edu/gems/

   This file is based upon a pre-release version of Multifacet GEMS
   from 2004 and may also contain additional modifications and code
   Copyright (C) 2004-2007 by Milo Martin for the Penn Architecture
   and Compilers Group.  Contact: acg@lists.seas.upenn.edu
   http://www.cis.upenn.edu/acg/

   --------------------------------------------------------------------

   Multifacet GEMS is free software; you can redistribute it and/or
   modify it under the terms of version 2 of the GNU General Public
   License as published by the Free Software Foundation.  This 
   software comes with ABSOLUTELY NO WARRANTY.  For details see the 
   file LICENSE included with the distribution.
*/
// -----------------------------------------------------------------------------
//
//  This file is part of FeS2.
//
//  FeS2 is free software: you can redistribute it and/or modify
//  it under the terms of the GNU General Public License as published by
//  the Free Software Foundation, either version 3 of the License, or
//  (at your option) any later version.
//
//  FeS2 is distributed in the hope that it will be useful,
//  but WITHOUT ANY WARRANTY; without even the implied warranty of
//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//  GNU General Public License for more details.
//
//  You should have received a copy of the GNU General Public License
//  along with FeS2.  If not, see <http://www.gnu.org/licenses/>.
//
// -----------------------------------------------------------------------------

machine(Directory, "Token protocol") {

  // STATES
  enumeration(State, desc="Directory states", default="Directory_State_O") {
    // Base states
    O, desc="Owner";
    NO, desc="Not Owner";
    L, desc="Locked";
  }

  // Events
  enumeration(Event, desc="Directory events") {
    GETX, desc="A GETX arrives";
    GETS, desc="A GETS arrives";
    Lockdown, desc="A lockdown request arrives";
    Unlockdown, desc="An un-lockdown request arrives";
    Data_Owner, desc="Data arrive, includes the owner token";
    Data_Shared, desc="Data arrive, does not include the owner token";
    Ack, desc="Tokens arrive";
    Ack_Owner, desc="Tokens arrive, including the owner token";
    Exclusive_Completion, desc="A exclusive completion arrives";
    Owned_Completion, desc="A owned completion arrives";
    Shared_Completion, desc="A shared completion arrives";
  }

  // TYPES

  // DirectoryEntry
  structure(Entry, desc="...") {
    State DirectoryState,          desc="Directory state";
    DataBlock DataBlk,             desc="data for the block";
    int Tokens, default="max_tokens()", desc="Number of tokens for the line we're holding";

    // The following state is provided to allow for bandwidth
    // efficient directory-like operation.  However all of this state
    // is 'soft state' that does not need to be correct (as long as
    // you're eventually willing to resort to broadcast.)

    //    Time LastOwnerUpdate,          desc="Timestamp of last update of the Owner information";
    //    Time LastSharersUpdate,        desc="Timestamp of last update of the Sharer information";
    Set Owner,                     desc="Probable Owner of the line.  More accurately, the set of processors who need to see a GetS or GetO.   We use a Set for convenience, but only one bit is set at a time.";
    Set Sharers,                   desc="Probable sharers of the line.  More accurately, the set of processors who need to see a GetX";
    Set Pending,                   desc="Set of recent requesters that have not send completions yet";
  }

  external_type(DirectoryMemory) {
    Entry lookup(Address);
    bool isPresent(Address);
  }

  // ** OBJECTS **

  DirectoryMemory directory;
  PersistentTable persistentTable;

  State getState(Address addr) {
    return directory[addr].DirectoryState;
  }
  
  void setState(Address addr, State state) {
    directory[addr].DirectoryState := state;
    
    if (state == State:L) {
      assert(directory[addr].Tokens == 0);
    }

    // We have one or zero owners
    assert((directory[addr].Owner.count() == 0) || (directory[addr].Owner.count() == 1));

    // Make sure the token count is in range
    assert(directory[addr].Tokens >= 0);
    assert(directory[addr].Tokens <= max_tokens());
    
    if (state == State:O) {
      assert(directory[addr].Tokens >= 1); // Must have at least one token
      assert(directory[addr].Tokens >= (max_tokens() / 2)); // Only mostly true; this might not always hold
    }
  }
  
  // ** BUFFERS **

  MessageBuffer requestFromDir, network="To", virtual_network="1", ordered="false";
  MessageBuffer responseFromDir, network="To", virtual_network="2", ordered="false";
  
  MessageBuffer persistentToDir, network="From", virtual_network="0", ordered="true";
  MessageBuffer requestToDir, network="From", virtual_network="1", ordered="false";
  MessageBuffer responseToDir, network="From", virtual_network="2", ordered="false";
  MessageBuffer completionToDir, network="From", virtual_network="3", ordered="false";

  // ** OUT_PORTS **
  out_port(responseNetwork_out, ResponseMsg, responseFromDir);
  out_port(requestNetwork_out, RequestMsg, requestFromDir);
  
  // ** IN_PORTS **

  in_port(persistentNetwork_in, PersistentMsg, persistentToDir) {
    if (persistentNetwork_in.isReady()) {
      peek(persistentNetwork_in, PersistentMsg) {

        // Apply the lockdown or unlockdown message to the table
        if (in_msg.Type == PersistentRequestType:GETX_PERSISTENT) {
          persistentTable.persistentRequestLock(in_msg.Address, in_msg.Requestor, AccessType:Write);
        } else if (in_msg.Type == PersistentRequestType:GETS_PERSISTENT) {
          persistentTable.persistentRequestLock(in_msg.Address, in_msg.Requestor, AccessType:Read);
        } else if (in_msg.Type == PersistentRequestType:DEACTIVATE_PERSISTENT) {
          persistentTable.persistentRequestUnlock(in_msg.Address, in_msg.Requestor);
        } else {
          error("Invalid message");
        }

        // React to the message based on the current state of the table
        if (persistentTable.isLocked(in_msg.Address)) {
          trigger(Event:Lockdown, in_msg.Address); // locked
        } else {
          trigger(Event:Unlockdown, in_msg.Address); // unlocked
        }
      }
    }
  }

  in_port(requestNetwork_in, RequestMsg, requestToDir) {
    if (requestNetwork_in.isReady()) {
      peek(requestNetwork_in, RequestMsg) {
        if (in_msg.Type == CoherenceRequestType:GETS) {
          trigger(Event:GETS, in_msg.Address);
        } else if (in_msg.Type == CoherenceRequestType:GETX) {
          trigger(Event:GETX, in_msg.Address);
        } else {
          error("Invalid message");
        }
      }
    }
  }

  in_port(responseNetwork_in, ResponseMsg, responseToDir) {
    if (responseNetwork_in.isReady()) {
      peek(responseNetwork_in, ResponseMsg) {
        if (in_msg.Type == CoherenceResponseType:DATA_OWNER) {
          trigger(Event:Data_Owner, in_msg.Address);
        } else if (in_msg.Type == CoherenceResponseType:ACK) {
          trigger(Event:Ack, in_msg.Address);
        } else if (in_msg.Type == CoherenceResponseType:DATA_SHARED) {
          trigger(Event:Data_Shared, in_msg.Address);
        } else if (in_msg.Type == CoherenceResponseType:ACK_OWNER) {
          trigger(Event:Ack_Owner, in_msg.Address);
        } else {
          error("Invalid message");
        }
      }
    }
  }
  
  in_port(completionNetwork_in, CompletionMsg, completionToDir) {
    if (completionNetwork_in.isReady()) {
      peek(completionNetwork_in, CompletionMsg) {
        if (in_msg.Result == CoherenceCompletionType:Owned) {
          trigger(Event:Owned_Completion, in_msg.Address);
        } else if (in_msg.Result == CoherenceCompletionType:Shared) {
          trigger(Event:Shared_Completion, in_msg.Address);
        } else if (in_msg.Result == CoherenceCompletionType:Exclusive) {
          trigger(Event:Exclusive_Completion, in_msg.Address);
        } else {
          error("Invalid message");
        }
      }
    }
  }

  // Actions
  
  action(a_sendTokens, "a", desc="Send tokens to requestor") {
    // Only send a message if we have tokens to send
    if (directory[address].Tokens > 0) {
      peek(requestNetwork_in, RequestMsg) {
        enqueue(responseNetwork_out, ResponseMsg, DIRECTORY_LATENCY) {
          out_msg.Address := address;
          out_msg.Type := CoherenceResponseType:ACK;
          out_msg.Sender := id;
          out_msg.SenderMachine := MachineType:Directory;
          out_msg.Destination.add(MachineType:L1Cache, in_msg.Requestor);
          out_msg.Tokens := directory[in_msg.Address].Tokens;
          out_msg.MessageSize := MessageSizeType:Response_Control;
        }
      }
      directory[address].Tokens := 0;
    }
  }

  action(aa_sendTokensToStarver, "\a", desc="Send tokens to starver") {
    // Only send a message if we have tokens to send
    if (directory[address].Tokens > 0) {
      enqueue(responseNetwork_out, ResponseMsg, DIRECTORY_LATENCY) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := id;
        out_msg.SenderMachine := MachineType:Directory;
        out_msg.Destination.add(MachineType:L1Cache, persistentTable.findSmallest(address));
        out_msg.Tokens := directory[address].Tokens;
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
      directory[address].Tokens := 0;
    }
  }

  action(b_forwardToSharers, "b", desc="Forward the incoming request to the sharers (and the owner)") {
    // FIXME - avoid null messages
    peek(requestNetwork_in, RequestMsg) {
      enqueue(requestNetwork_out, RequestMsg, DIRECTORY_LATENCY) {
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination.addSet(MachineType:L1Cache, directory[address].Sharers);
        out_msg.Destination.addSet(MachineType:L1Cache, directory[address].Owner);
        out_msg.Destination.addSet(MachineType:L1Cache, directory[address].Pending);
        out_msg.Destination.remove(MachineType:L1Cache, in_msg.Requestor); // Don't send it back to the requestor
        out_msg.Destination.removeNetDest(in_msg.Destination); // Remove the caches that already saw the request

        // Note: we don't send it back to the directory because it could cause a loop

        out_msg.MessageSize := MessageSizeType:Forwarded_Control;
      }
    }
  }

  action(c_forwardToOwner, "c", desc="Forward the incoming request to the owner") {
    // FIXME, avoid null messages
    peek(requestNetwork_in, RequestMsg) {
      enqueue(requestNetwork_out, RequestMsg, DIRECTORY_LATENCY) {
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination.addSet(MachineType:L1Cache, directory[address].Owner);
        out_msg.Destination.addSet(MachineType:L1Cache, directory[address].Pending);
        out_msg.Destination.remove(MachineType:L1Cache, in_msg.Requestor); // Don't send it back to the requestor
        out_msg.Destination.removeNetDest(in_msg.Destination); // Remove the caches that already saw the request

        // Note: we don't send it back to the directory because it could cause a loop

        out_msg.MessageSize := MessageSizeType:Forwarded_Control;
      }
    }
  }

  action(d_sendDataWithAllTokens, "d", desc="Send data and tokens to requestor") {
    peek(requestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, MEMORY_LATENCY) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_OWNER;
        out_msg.Sender := id;
        out_msg.SenderMachine := MachineType:Directory;
        out_msg.Destination.add(MachineType:L1Cache, in_msg.Requestor);
        assert(directory[address].Tokens > 0);
        out_msg.Tokens := directory[in_msg.Address].Tokens;
        out_msg.DataBlk := directory[in_msg.Address].DataBlk;
        out_msg.Dirty := false;
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
    directory[address].Tokens := 0;
  }

  action(dd_sendDataWithAllTokensToStarver, "\d", desc="Send data and tokens to starver") {
    enqueue(responseNetwork_out, ResponseMsg, MEMORY_LATENCY) {
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:DATA_OWNER;
      out_msg.Sender := id;
      out_msg.SenderMachine := MachineType:Directory;
      out_msg.Destination.add(MachineType:L1Cache, persistentTable.findSmallest(address));
      assert(directory[address].Tokens > 0);
      out_msg.Tokens := directory[address].Tokens;
      out_msg.DataBlk := directory[address].DataBlk;
      out_msg.Dirty := false;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
    directory[address].Tokens := 0;
  }

  action(f_incrementTokens, "f", desc="Increment the number of tokens we're tracking") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(in_msg.Tokens >= 1);
      directory[address].Tokens := directory[address].Tokens + in_msg.Tokens;
    }
  }

  action(i_popIncomingCompletionQueue, "i", desc="Pop incoming completion queue") {
    completionNetwork_in.dequeue();
  }

  action(j_popIncomingRequestQueue, "j", desc="Pop incoming request queue") {
    peek(requestNetwork_in, RequestMsg) {
      directory[address].Pending.add(in_msg.Requestor);
    }
    requestNetwork_in.dequeue();
  }

  action(k_popIncomingResponseQueue, "k", desc="Pop incoming response queue") {
    responseNetwork_in.dequeue();
  }

  action(l_popIncomingPersistentQueue, "l", desc="Pop incoming persistent queue") {
    peek(persistentNetwork_in, PersistentMsg) {
      directory[address].Pending.add(in_msg.Requestor);
    }
    persistentNetwork_in.dequeue();
  }

  action(m_writeDataToMemory, "m", desc="Write dirty writeback to memory") {
    peek(responseNetwork_in, ResponseMsg) {
      directory[in_msg.Address].DataBlk := in_msg.DataBlk;
      DEBUG_EXPR(in_msg.Address);
      DEBUG_EXPR(in_msg.DataBlk);
    }
  }

  action(n_checkIncomingMsg, "n", desc="Check incoming token message") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(in_msg.Type == CoherenceResponseType:ACK_OWNER);
      assert(in_msg.Dirty == false);
      assert(in_msg.MessageSize == MessageSizeType:Writeback_Control);
      assert(directory[in_msg.Address].DataBlk == in_msg.DataBlk);
    }
  }

  action(p_completionOwnerWriteback, "p", desc="Completion of an owner token writeback") {
    peek(responseNetwork_in, ResponseMsg) {

      directory[address].Owner.clear();
      directory[address].Sharers.remove(in_msg.Sender);
      directory[address].Pending.remove(in_msg.Sender);

//       // Update owner
//       directory[address].Owner.clear();
//       directory[address].LastOwnerUpdate := get_time();

//       // Update sharers
//       if (time_to_int(get_time()) >= time_to_int(directory[address].LastSharersUpdate)) {
//         directory[address].Sharers.remove(in_msg.Sender);
//       }
      
      // Check for all tokens
      if (directory[address].Tokens == max_tokens()) {
        directory[address].Sharers.clear();
        directory[address].Owner.clear();
        directory[address].Pending.clear();
      }
    }    
  }

  action(q_completionCleanWriteback, "q", desc="Completion of a clean writeback") {
    peek(responseNetwork_in, ResponseMsg) {

      directory[address].Sharers.remove(in_msg.Sender);
      directory[address].Pending.remove(in_msg.Sender);

//       // Update sharers
//       if (time_to_int(get_time()) >= time_to_int(directory[address].LastSharersUpdate)) {
//         directory[address].Sharers.remove(in_msg.Sender);
//       }

      // Check for all tokens
      if (directory[address].Tokens == max_tokens()) {
        directory[address].Sharers.clear();
        directory[address].Owner.clear();
        directory[address].Pending.clear();
      }
    }    
  }

  action(r_bounceResponse, "r", desc="Bounce response to starving processor") {
    peek(responseNetwork_in, ResponseMsg) {
      enqueue(responseNetwork_out, ResponseMsg, NULL_LATENCY) {
        out_msg.Address := address;
        out_msg.Type := in_msg.Type;
        out_msg.Sender := id;
        out_msg.SenderMachine := MachineType:Directory;
        out_msg.Destination.add(MachineType:L1Cache, persistentTable.findSmallest(address));
        out_msg.Tokens := in_msg.Tokens;
        out_msg.DataBlk := in_msg.DataBlk;
        out_msg.Dirty := in_msg.Dirty;
        out_msg.MessageSize := in_msg.MessageSize;
      }
    }
  }

  action(s_bounceDatalessOwnerToken, "s", desc="Bounce clean owner token to starving processor") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(in_msg.Type == CoherenceResponseType:ACK_OWNER);
      assert(in_msg.Dirty == false);
      assert(in_msg.MessageSize == MessageSizeType:Writeback_Control);

      // NOTE: The following check would not be valid in a real
      // implementation.  We include the data in the "dataless"
      // message so we can assert the clean data matches the datablock
      // in memory
      assert(directory[in_msg.Address].DataBlk == in_msg.DataBlk);

      // Bounce the message, but "re-associate" the data and the owner
      // token.  In essence we're converting an ACK_OWNER message to a
      // DATA_OWNER message, keeping the number of tokens the same.
      enqueue(responseNetwork_out, ResponseMsg, NULL_LATENCY) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_OWNER;
        out_msg.Sender := id;
        out_msg.SenderMachine := MachineType:Directory;
        out_msg.Destination.add(MachineType:L1Cache, persistentTable.findSmallest(address));
        out_msg.Tokens := in_msg.Tokens;
        out_msg.DataBlk := directory[in_msg.Address].DataBlk;
        out_msg.Dirty := in_msg.Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(v_completionExclusive, "v", desc="") {
    peek(completionNetwork_in, CompletionMsg) {
      
      directory[address].Owner.clear();
      directory[address].Owner.add(in_msg.Sender);

      directory[address].Sharers.clear();
      directory[address].Sharers.add(in_msg.Sender);

      directory[address].Pending.remove(in_msg.Sender);

//       // Update owner
//       if (time_to_int(in_msg.Timestamp) >= time_to_int(directory[address].LastOwnerUpdate)) {
//         directory[address].Owner.clear();
//         directory[address].Owner.add(in_msg.Sender);
//         directory[address].LastOwnerUpdate := in_msg.Timestamp;
//       }

//       // Update sharers
//       if (time_to_int(in_msg.Timestamp) >= time_to_int(directory[address].LastSharersUpdate)) {
//         directory[address].Sharers.clear();
//         directory[address].LastSharersUpdate := in_msg.Timestamp;
//       }
    }
  }

  action(w_completionOwned, "w", desc="") {
    peek(completionNetwork_in, CompletionMsg) {

      directory[address].Owner.clear();
      directory[address].Owner.add(in_msg.Sender);

      directory[address].Sharers.add(in_msg.Sender);

      directory[address].Pending.remove(in_msg.Sender);


//       // Update owner
//       if (time_to_int(in_msg.Timestamp) >= time_to_int(directory[address].LastOwnerUpdate)) {
//         directory[address].Owner.clear();
//         directory[address].Owner.add(in_msg.Sender);
//         directory[address].LastOwnerUpdate := in_msg.Timestamp;
//       }

//       // Update sharers
//       directory[address].Sharers.add(in_msg.Sender);
//       if (time_to_int(in_msg.Timestamp) >= time_to_int(directory[address].LastSharersUpdate)) {
//         directory[address].LastSharersUpdate := in_msg.Timestamp;
//       }
    }
  }

  action(x_completionShared, "x", desc="") {
    peek(completionNetwork_in, CompletionMsg) {

      directory[address].Sharers.add(in_msg.Sender);
      directory[address].Pending.remove(in_msg.Sender);

//       // Update sharers
//       directory[address].Sharers.add(in_msg.Sender);
//       if (time_to_int(in_msg.Timestamp) >= time_to_int(directory[address].LastSharersUpdate)) {
//         directory[address].LastSharersUpdate := in_msg.Timestamp;
//       }
    }
  }
    
  // TRANSITIONS

  // Trans. from O
  transition(O, GETX, NO) {
    d_sendDataWithAllTokens;
    b_forwardToSharers;
    j_popIncomingRequestQueue;
  }

  transition(O, GETS, NO) {
    d_sendDataWithAllTokens;
    // Since we found the owner, no need to forward
    j_popIncomingRequestQueue;
  }

  transition(O, Lockdown, L) {
    dd_sendDataWithAllTokensToStarver;
    l_popIncomingPersistentQueue;
  }

  transition(O, {Data_Shared, Ack}) {
    f_incrementTokens;
    q_completionCleanWriteback;
    k_popIncomingResponseQueue;
  }

  // Trans. from NO
  transition(NO, GETX) {
    a_sendTokens;
    b_forwardToSharers;
    j_popIncomingRequestQueue;
  }

  transition(NO, GETS) {
    c_forwardToOwner;
    j_popIncomingRequestQueue;
  }

  transition(NO, Lockdown, L) {
    aa_sendTokensToStarver;
    l_popIncomingPersistentQueue;
  }

  transition(NO, Data_Owner, O) {
    m_writeDataToMemory;
    f_incrementTokens;
    p_completionOwnerWriteback;
    k_popIncomingResponseQueue;
  }

  transition(NO, Ack_Owner, O) {
    n_checkIncomingMsg;
    f_incrementTokens;
    p_completionOwnerWriteback;
    k_popIncomingResponseQueue;
  }

  transition(NO, {Data_Shared, Ack}) {
    f_incrementTokens;
    q_completionCleanWriteback;
    k_popIncomingResponseQueue;
  }

  // Trans. from L
  transition(L, {GETX, GETS}) {
    j_popIncomingRequestQueue;
  }

  transition(L, Lockdown) {
    l_popIncomingPersistentQueue;
  }

  transition(L, {Data_Owner, Data_Shared, Ack}) {
    r_bounceResponse;
    k_popIncomingResponseQueue;
  }

  transition(L, Ack_Owner) {
    s_bounceDatalessOwnerToken;
    k_popIncomingResponseQueue;
  }

  transition(L, Unlockdown, NO) {
    l_popIncomingPersistentQueue;
  }

  // Completion Messages
  transition({O, NO, L}, Exclusive_Completion) {
    v_completionExclusive;
    i_popIncomingCompletionQueue;
  }

  transition({O, NO, L}, Owned_Completion) {
    w_completionOwned;
    i_popIncomingCompletionQueue;
  }

  transition({O, NO, L}, Shared_Completion) {
    x_completionShared;
    i_popIncomingCompletionQueue;
  }
}
